{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "81a4bfcd",
      "metadata": {
        "id": "81a4bfcd"
      },
      "source": [
        "# MNIST CNN Improvement Workshop - Starter Notebook\n",
        "## From Lesson 3.7 Neural Network & Deep Learning\n",
        "\n",
        "This notebook provides the foundation code for implementing CNN improvements on MNIST digit classification.\n",
        "\n",
        "**Baseline Goal:** Start with MLP achieving ~97% accuracy  \n",
        "**Workshop Goal:** Achieve 99%+ accuracy using CNN techniques\n",
        "\n",
        "Please use Conda DL environment with PyTorch installed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e45773f",
      "metadata": {
        "id": "9e45773f"
      },
      "source": [
        "## 1. Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "74e3782d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74e3782d",
        "outputId": "a4ef4c2b-d080-4ec4-b3c7-63419a75510c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "CUDA device name: Tesla T4\n",
            "CUDA memory: 14.7 GB\n"
          ]
        }
      ],
      "source": [
        "# Import libraries for deep learning and data handling\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "# Reproducibility: fix random seeds for comparable runs\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "# Check for CUDA availability and use GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "if torch.cuda.is_available():\n",
        "    print(f'CUDA device name: {torch.cuda.get_device_name()}')\n",
        "    print(f'CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc4e0800",
      "metadata": {
        "id": "bc4e0800"
      },
      "source": [
        "## 2. Load MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0c94e402",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c94e402",
        "lines_to_next_cell": 1,
        "outputId": "0a889419-3ecc-4323-c5d1-797dda1b827f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 16.1MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 483kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.45MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 9.52MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training samples: 60000\n",
            "Test samples: 10000\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Define data preprocessing: train with augmentation, test without\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomRotation(10),  # Mild rotation for digits (no horizontal flip for 6/9)\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n",
        "])\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Load training data (60,000 images) with augmentation\n",
        "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform_train)\n",
        "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Load test data (10,000 images) without augmentation\n",
        "testset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform_test)\n",
        "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
        "\n",
        "print(f'Training samples: {len(trainset)}')\n",
        "print(f'Test samples: {len(testset)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a51199c5",
      "metadata": {
        "id": "a51199c5"
      },
      "source": [
        "## 3. Helper Functions for Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "23823a93",
      "metadata": {
        "id": "23823a93",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def train_model(model, trainloader, criterion, optimizer, scheduler=None, epochs=5):\n",
        "    \"\"\"Train a model and return training history\"\"\"\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for images, labels in trainloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        epoch_loss = running_loss / len(trainloader)\n",
        "        train_losses.append(epoch_loss)\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}')\n",
        "\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "    return train_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "bf25e88c",
      "metadata": {
        "id": "bf25e88c",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, testloader):\n",
        "    \"\"\"Evaluate model and return accuracy\"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2e575a93",
      "metadata": {
        "id": "2e575a93",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    \"\"\"Count total trainable parameters in model\"\"\"\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def validate_model_architecture(model):\n",
        "    \"\"\"Validate student model meets requirements\"\"\"\n",
        "    param_count = count_parameters(model)\n",
        "    print(f\"Model has {param_count:,} trainable parameters\")\n",
        "\n",
        "    # Optional: layer type summary to catch obvious mistakes\n",
        "    layer_types = [type(m).__name__ for m in model.modules() if len(list(m.children())) == 0 and type(m).__name__ != \"Sequential\"]\n",
        "    print(f\"Layer types: {', '.join(layer_types[:12])}{'...' if len(layer_types) > 12 else ''}\")\n",
        "\n",
        "    # Sanity check: model should have a reasonable number of parameters (e.g. at least 1k)\n",
        "    min_params = 1000\n",
        "    assert param_count >= min_params, f\"Model has too few parameters ({param_count:,}); expected at least {min_params:,}\"\n",
        "\n",
        "    # Test forward pass\n",
        "    test_input = torch.randn(1, 1, 28, 28).to(device)\n",
        "    try:\n",
        "        output = model(test_input)\n",
        "        assert output.shape == (1, 10), f\"Expected output shape (1, 10), got {output.shape}\"\n",
        "        print(\"✓ Model architecture validation passed\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Model architecture validation failed: {e}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc0f14a1",
      "metadata": {
        "id": "dc0f14a1"
      },
      "source": [
        "## 4. Baseline Model - From Lesson 3.7 Neural Network & Deep Learning\n",
        "\n",
        "This is our starting point: a Multi-Layer Perceptron (MLP) that achieves approximately 97% accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3bf0823f",
      "metadata": {
        "id": "3bf0823f",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "class BaselineMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BaselineMLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.shape[0], -1)  # Flatten\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ff14102",
      "metadata": {
        "id": "8ff14102"
      },
      "source": [
        "## 5. Train and Evaluate Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "74010e9f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74010e9f",
        "lines_to_next_cell": 1,
        "outputId": "f14af02f-ac94-4543-fc19-a746faa8b052"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Baseline MLP (from Lesson 3.7 Neural Network & Deep Learning)...\n",
            "Model has 109,386 trainable parameters\n",
            "✓ Model architecture validation passed\n",
            "Epoch 1/5, Loss: 0.2331\n",
            "Epoch 2/5, Loss: 0.1128\n",
            "Epoch 3/5, Loss: 0.0893\n",
            "Epoch 4/5, Loss: 0.0768\n",
            "Epoch 5/5, Loss: 0.0668\n",
            "\n",
            "Baseline MLP Accuracy: 97.03%\n"
          ]
        }
      ],
      "source": [
        "print(\"Training Baseline MLP (from Lesson 3.7 Neural Network & Deep Learning)...\")\n",
        "baseline_model = BaselineMLP().to(device)\n",
        "baseline_criterion = nn.NLLLoss()\n",
        "baseline_optimizer = optim.Adam(baseline_model.parameters(), lr=0.003)\n",
        "\n",
        "# Validate architecture\n",
        "validate_model_architecture(baseline_model)\n",
        "\n",
        "# Train the model\n",
        "baseline_losses = train_model(baseline_model, trainloader, baseline_criterion, baseline_optimizer)\n",
        "\n",
        "# Evaluate the model\n",
        "baseline_accuracy = evaluate_model(baseline_model, testloader)\n",
        "print(f'\\nBaseline MLP Accuracy: {baseline_accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a698da9d",
      "metadata": {
        "id": "a698da9d"
      },
      "source": [
        "## 6. Improvement Exploration\n",
        "\n",
        "Now that you have a working baseline achieving ~97% accuracy, explore ways to improve performance.\n",
        "\n",
        "**Possible improvement paths:**\n",
        "\n",
        "Architecture Changes, Regularization Approaches, Training Enhancements, Data Augmentation, Others"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1691d20",
      "metadata": {
        "id": "d1691d20"
      },
      "source": [
        "## 7. Your Implementation Area\n",
        "\n",
        "Use the cells below to implement your chosen improvement techniques:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c3c13409",
      "metadata": {
        "id": "c3c13409"
      },
      "outputs": [],
      "source": [
        "# Implement your improved model here\n",
        "class ImprovedModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImprovedModel, self).__init__()\n",
        "        # Convolutional layer with batch normalization (MNIST is 1 channel grayscale)\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(num_features=32)\n",
        "        # Max pooling layer\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # Second convolutional layer with batch normalization\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(num_features=64)\n",
        "        # Fully connected layers with dropout for regularization\n",
        "        self.fc1 = nn.Linear(in_features=64 * 7 * 7, out_features=512)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "        self.fc2 = nn.Linear(in_features=512, out_features=10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply convolutional layers, activation function, and pooling\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        # Flatten the tensor for the fully connected layers\n",
        "        x = x.view(-1, 64 * 7 * 7)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)  # NLLLoss expects log probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a890f2b",
      "metadata": {
        "id": "5a890f2b"
      },
      "outputs": [],
      "source": [
        "# Test your implementation here\n",
        "model = ImprovedModel().to(device)\n",
        "validate_model_architecture(model)\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "\n",
        "print(\"Training your improved model...\")\n",
        "losses = train_model(model, trainloader, criterion, optimizer, scheduler=scheduler, epochs=10)\n",
        "accuracy = evaluate_model(model, testloader)\n",
        "print(f'Your Model Accuracy: {accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26146c35",
      "metadata": {
        "id": "26146c35"
      },
      "source": [
        "## 8. Results Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f12ff93",
      "metadata": {
        "id": "8f12ff93"
      },
      "outputs": [],
      "source": [
        "# Compare your results with the baseline\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ACCURACY COMPARISON\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Baseline MLP:        {baseline_accuracy:.2f}%\")\n",
        "print(f\"Your Model:          {accuracy:.2f}%\")\n",
        "print(f\"Improvement:         +{accuracy - baseline_accuracy:.2f}%\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5438aa2e",
      "metadata": {
        "id": "5438aa2e",
        "lines_to_next_cell": 3
      },
      "source": [
        "## 9. Next Steps\n",
        "\n",
        "**Experiment and iterate:**\n",
        "- Try different combinations of techniques\n",
        "- Analyze what works and what doesn't\n",
        "- Compare training time vs. accuracy trade-offs\n",
        "- Document your findings and insights\n",
        "\n",
        "**Remember:** The goal is to understand how different techniques contribute to improved performance!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86231cad",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment 3: Data Augmentation - Target: ~99%\n",
        "# Enhanced transforms for training\n",
        "transform_augmented = transforms.Compose([\n",
        "   transforms.RandomRotation(10),\n",
        "   transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "   transforms.ToTensor(),\n",
        "   transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Create augmented training dataset\n",
        "trainset_aug = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform_augmented)\n",
        "trainloader_aug = DataLoader(trainset_aug, batch_size=64, shuffle=True)\n",
        "\n",
        "# Same architecture as Exercise 2\n",
        "class AugmentedCNN(nn.Module):\n",
        "   def __init__(self):\n",
        "       super(AugmentedCNN, self).__init__()\n",
        "       self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "       self.bn1 = nn.BatchNorm2d(32)\n",
        "       self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "       self.bn2 = nn.BatchNorm2d(64)\n",
        "       self.pool = nn.MaxPool2d(2, 2)\n",
        "       self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "       self.dropout = nn.Dropout(0.25)\n",
        "       self.fc2 = nn.Linear(128, 10)\n",
        "      \n",
        "   def forward(self, x):\n",
        "       x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "       x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "       x = x.view(x.size(0), -1)\n",
        "       x = F.relu(self.fc1(x))\n",
        "       x = self.dropout(x)\n",
        "       x = self.fc2(x)\n",
        "       return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cbc17b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test your implementation here\n",
        "model = AugmentedCNN().to(device)\n",
        "validate_model_architecture(model)\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
        "\n",
        "print(\"Training your improved model...\")\n",
        "losses = train_model(model, trainloader, criterion, optimizer)\n",
        "accuracy = evaluate_model(model, testloader)\n",
        "print(f'Your Model Accuracy: {accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f97c2ab",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment 4: Learning Rate Scheduling - Target: ~99.2%\n",
        "class ScheduledCNN(nn.Module):\n",
        "   def __init__(self):\n",
        "       super(ScheduledCNN, self).__init__()\n",
        "       self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "       self.bn1 = nn.BatchNorm2d(32)\n",
        "       self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "       self.bn2 = nn.BatchNorm2d(64)\n",
        "       self.pool = nn.MaxPool2d(2, 2)\n",
        "       self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "       self.dropout = nn.Dropout(0.25)\n",
        "       self.fc2 = nn.Linear(128, 10)\n",
        "      \n",
        "   def forward(self, x):\n",
        "       x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "       x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "       x = x.view(x.size(0), -1)\n",
        "       x = F.relu(self.fc1(x))\n",
        "       x = self.dropout(x)\n",
        "       x = self.fc2(x)\n",
        "       return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ecb0901",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train CNN with Learning Rate Scheduling\n",
        "print(\"Training CNN with Learning Rate Scheduling...\")\n",
        "cnn4_model = ScheduledCNN().to(device)\n",
        "cnn4_criterion = nn.NLLLoss()\n",
        "cnn4_optimizer = optim.Adam(cnn4_model.parameters(), lr=0.01)  # Higher initial LR\n",
        "cnn4_scheduler = optim.lr_scheduler.StepLR(cnn4_optimizer, step_size=2, gamma=0.7)\n",
        "\n",
        "\n",
        "cnn4_losses = train_model(cnn4_model, trainloader_aug, cnn4_criterion, cnn4_optimizer, cnn4_scheduler, epochs=5)\n",
        "cnn4_accuracy = evaluate_model(cnn4_model, testloader)\n",
        "print(f'Scheduled CNN Accuracy: {cnn4_accuracy:.2f}%\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62edb499",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment 5: Advanced ResNet-style Architecture - Target: ~99.3%+\n",
        "class ResidualBlock(nn.Module):\n",
        "   def __init__(self, in_channels, out_channels, stride=1):\n",
        "       super(ResidualBlock, self).__init__()\n",
        "       self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "       self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "       self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "       self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "      \n",
        "       self.shortcut = nn.Sequential()\n",
        "       if stride != 1 or in_channels != out_channels:\n",
        "           self.shortcut = nn.Sequential(\n",
        "               nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "               nn.BatchNorm2d(out_channels)\n",
        "           )\n",
        "  \n",
        "   def forward(self, x):\n",
        "       out = F.relu(self.bn1(self.conv1(x)))\n",
        "       out = self.bn2(self.conv2(out))\n",
        "       out += self.shortcut(x)\n",
        "       out = F.relu(out)\n",
        "       return out\n",
        "\n",
        "\n",
        "class AdvancedCNN(nn.Module):\n",
        "   def __init__(self):\n",
        "       super(AdvancedCNN, self).__init__()\n",
        "       self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1, bias=False)\n",
        "       self.bn1 = nn.BatchNorm2d(32)\n",
        "      \n",
        "       # Residual blocks\n",
        "       self.layer1 = ResidualBlock(32, 64, stride=2)  # 28x28 -> 14x14\n",
        "       self.layer2 = ResidualBlock(64, 128, stride=2)  # 14x14 -> 7x7\n",
        "      \n",
        "       # Global average pooling\n",
        "       self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "      \n",
        "       # Classifier\n",
        "       self.fc = nn.Linear(128, 10)\n",
        "      \n",
        "   def forward(self, x):\n",
        "       x = F.relu(self.bn1(self.conv1(x)))\n",
        "       x = self.layer1(x)\n",
        "       x = self.layer2(x)\n",
        "       x = self.avgpool(x)\n",
        "       x = x.view(x.size(0), -1)\n",
        "       x = self.fc(x)\n",
        "       return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "# Train Advanced ResNet-style CNN\n",
        "print(\"Training Advanced ResNet-style CNN...\")\n",
        "cnn5_model = AdvancedCNN().to(device)\n",
        "cnn5_criterion = nn.NLLLoss()\n",
        "cnn5_optimizer = optim.Adam(cnn5_model.parameters(), lr=0.001)\n",
        "cnn5_scheduler = optim.lr_scheduler.StepLR(cnn5_optimizer, step_size=2, gamma=0.8)\n",
        "\n",
        "\n",
        "cnn5_losses = train_model(cnn5_model, trainloader_aug, cnn5_criterion, cnn5_optimizer, cnn5_scheduler)\n",
        "cnn5_accuracy = evaluate_model(cnn5_model, testloader)\n",
        "print(f'Advanced CNN Accuracy: {cnn5_accuracy:.2f}%\\n')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "dl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
